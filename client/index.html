<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Qwen 2.5 Omni Test Client</title>
    <style>
      body {
        font-family: Arial, sans-serif;
        max-width: 800px;
        margin: 0 auto;
        padding: 20px;
      }
      .container {
        display: flex;
        flex-direction: column;
        gap: 20px;
      }
      .status {
        padding: 10px;
        border-radius: 5px;
        margin-bottom: 20px;
      }
      .connected {
        background-color: #d4edda;
        color: #155724;
      }
      .disconnected {
        background-color: #f8d7da;
        color: #721c24;
      }
      .loading {
        background-color: #fff3cd;
        color: #856404;
      }
      textarea {
        width: 100%;
        height: 100px;
        padding: 10px;
        margin-bottom: 10px;
      }
      .media-input {
        display: flex;
        flex-wrap: wrap;
        gap: 10px;
        margin-bottom: 20px;
      }
      .media-preview {
        display: flex;
        flex-wrap: wrap;
        gap: 10px;
        margin-top: 10px;
      }
      .preview-item {
        position: relative;
        width: 150px;
        height: 150px;
        border: 1px solid #ddd;
        border-radius: 5px;
        overflow: hidden;
      }
      .preview-item img {
        width: 100%;
        height: 100%;
        object-fit: cover;
      }
      .preview-item .remove {
        position: absolute;
        top: 5px;
        right: 5px;
        background: rgba(255, 255, 255, 0.7);
        border-radius: 50%;
        width: 24px;
        height: 24px;
        text-align: center;
        line-height: 24px;
        cursor: pointer;
      }
      .response {
        border: 1px solid #ddd;
        border-radius: 5px;
        padding: 20px;
        background-color: #f9f9f9;
        min-height: 100px;
      }
      .controls {
        display: flex;
        gap: 10px;
        align-items: center;
      }
      button {
        padding: 10px 15px;
        border: none;
        border-radius: 5px;
        background-color: #007bff;
        color: white;
        cursor: pointer;
      }
      button:hover {
        background-color: #0069d9;
      }
      button:disabled {
        background-color: #cccccc;
        cursor: not-allowed;
      }
      select {
        padding: 10px;
        border-radius: 5px;
        border: 1px solid #ddd;
      }
      .toggle {
        display: flex;
        align-items: center;
        gap: 5px;
      }
      .audio-player {
        width: 100%;
        margin-top: 10px;
      }
      .metrics {
        margin-top: 10px;
        font-size: 0.9em;
        color: #6c757d;
      }
    </style>
  </head>
  <body>
    <div class="container">
      <h1>Qwen 2.5 Omni Test Client</h1>

      <div id="status" class="status disconnected">Connecting to server...</div>

      <div id="model-info" style="display: none">
        <h3>Model Information</h3>
        <pre id="model-info-json"></pre>
      </div>

      <div id="input-section">
        <h3>Input</h3>
        <textarea
          id="system-prompt"
          placeholder="(Optional) You are a helpful assistant."
          rows="3"
        ></textarea>
        <textarea
          id="text-input"
          placeholder="Enter your text prompt here..."
        ></textarea>

        <div class="media-input">
          <div>
            <label for="image-upload">Images:</label>
            <input type="file" id="image-upload" accept="image/*" multiple />
          </div>
          <div>
            <label for="audio-upload">Audio:</label>
            <input type="file" id="audio-upload" accept="audio/*" multiple />
          </div>
          <div>
            <label for="video-upload">Video:</label>
            <input type="file" id="video-upload" accept="video/*" multiple />
          </div>
        </div>
        <div>
          <button id="record-button" type="button">Record Audio</button>
          <span id="recording-status"></span>
        </div>

        <div class="media-preview" id="media-preview">
          <!-- Media previews will be added here -->
        </div>

        <div class="controls">
          <button id="send-button" disabled>Send</button>

          <div class="toggle">
            <label for="audio-output">Audio Output:</label>
            <input type="checkbox" id="audio-output" checked />
          </div>

          <div>
            <label for="speaker-select">Speaker:</label>
            <select id="speaker-select">
              <option value="Chelsie">Chelsie</option>
              <option value="Ethan">Ethan</option>
              <option value="Cherry">Cherry</option>
            </select>
          </div>
        </div>
      </div>

      <div id="response-section">
        <h3>Response</h3>
        <div class="response" id="text-response">
          Responses will appear here...
        </div>

        <div id="audio-response" style="display: none">
          <h4>Audio Response</h4>
          <audio id="audio-player" controls class="audio-player"></audio>
        </div>

        <div class="metrics" id="metrics">
          <!-- Performance metrics will be displayed here -->
        </div>
        <div class="controls">
          <button id="send-button" disabled>Send</button>

          <div class="toggle">
            <label for="audio-output">Audio Output:</label>
            <input type="checkbox" id="audio-output" checked />
          </div>

          <div>
            <label for="speaker-select">Speaker:</label>
            <select id="speaker-select">
              <option value="Chelsie">Chelsie</option>
              <option value="Ethan">Ethan</option>
              <option value="Cherry">Cherry</option>
            </select>
          </div>
        </div>

        <div
          class="webrtc-section"
          style="
            margin-top: 30px;
            padding: 20px;
            border: 1px solid #ddd;
            border-radius: 5px;
          "
        >
          <h3>Real-time Audio Streaming (WebRTC)</h3>
          <div id="webrtc-status" class="status disconnected">
            Not connected
          </div>

          <div
            class="visualizer-container"
            style="
              width: 100%;
              height: 100px;
              margin: 20px 0;
              background-color: #f0f0f0;
              border-radius: 5px;
            "
          >
            <canvas id="audio-visualizer" width="800" height="100"></canvas>
          </div>

          <div
            class="webrtc-controls"
            style="display: flex; gap: 10px; margin-bottom: 20px"
          >
            <button id="webrtc-start-button">Start Streaming</button>
            <button id="webrtc-stop-button" disabled>Stop Streaming</button>
          </div>

          <div
            id="webrtc-responses"
            style="
              max-height: 300px;
              overflow-y: auto;
              border: 1px solid #ddd;
              padding: 10px;
              border-radius: 5px;
            "
          >
            <div style="color: #666; font-style: italic">
              Real-time inference results will appear here...
            </div>
          </div>
        </div>
      </div>
    </div>

    <script>
      const API_BASE_URL = "http://100.88.244.124:80";
      const API_ENDPOINTS = {
        health: `${API_BASE_URL}/health`,
        speakers: `${API_BASE_URL}/api/inference/speakers`,
        inference: `${API_BASE_URL}/api/inference/`,
        multimodal: `${API_BASE_URL}/api/inference/multimodal`,
      };
      let modelInfo = null;
      let mediaFiles = {
        images: [],
        audios: [],
        videos: [],
      };

      // Check server connection and load model info
      async function checkConnection() {
        console.log("Checking connection to:", API_ENDPOINTS.health);

        const statusElement = document.getElementById("status");
        const sendButton = document.getElementById("send-button");

        try {
          // Check health endpoint
          const healthResponse = await fetch(API_ENDPOINTS.health);
          const healthData = await healthResponse.json();

          if (healthData.status === "healthy" && healthData.model_loaded) {
            statusElement.className = "status connected";
            statusElement.textContent =
              "Connected to server. Model loaded successfully.";
            sendButton.disabled = false;

            // Get speakers for model info
            await loadModelInfo();
          } else {
            statusElement.className = "status loading";
            statusElement.textContent =
              "Server is up, but model is still loading...";
            sendButton.disabled = true;

            // Check again in 10 seconds
            setTimeout(checkConnection, 10000);
          }
        } catch (error) {
          statusElement.className = "status disconnected";
          statusElement.textContent = `Server connection error: ${error.message}. Retrying in 10 seconds...`;
          sendButton.disabled = true;

          // Try again in 10 seconds
          setTimeout(checkConnection, 10000);
        }
      }

      // Load model information
      async function loadModelInfo() {
        try {
          const response = await fetch(`${API_BASE_URL}/model/info`);
          modelInfo = await response.json();

          // Display model info
          const modelInfoElement = document.getElementById("model-info");
          const modelInfoJson = document.getElementById("model-info-json");
          modelInfoElement.style.display = "block";
          modelInfoJson.textContent = JSON.stringify(modelInfo, null, 2);

          // Populate speaker dropdown
          const speakerSelect = document.getElementById("speaker-select");
          speakerSelect.innerHTML = "";

          const speakers = ["Chelsie", "Ethan", "Cherry"];
          speakers.forEach((speaker) => {
            const option = document.createElement("option");
            option.value = speaker;
            option.textContent = speaker;
            speakerSelect.appendChild(option);
          });

          // Set audio output checkbox based on model
          const audioOutputCheckbox = document.getElementById("audio-output");
          audioOutputCheckbox.checked = true;
        } catch (error) {
          console.error("Error loading model info:", error);
        }
      }

      // Handle file uploads for images
      document
        .getElementById("image-upload")
        .addEventListener("change", (event) => {
          handleFileUpload(event.target.files, "images");
        });

      // Handle file uploads for audio
      document
        .getElementById("audio-upload")
        .addEventListener("change", (event) => {
          handleFileUpload(event.target.files, "audios");
        });

      // Handle file uploads for video
      document
        .getElementById("video-upload")
        .addEventListener("change", (event) => {
          handleFileUpload(event.target.files, "videos");
        });

      // Process uploaded files and create previews
      function handleFileUpload(files, type) {
        if (!files || files.length === 0) return;

        for (let i = 0; i < files.length; i++) {
          const file = files[i];
          mediaFiles[type].push(file);

          // Create preview element
          const previewContainer = document.createElement("div");
          previewContainer.className = "preview-item";
          previewContainer.dataset.type = type;
          previewContainer.dataset.index = mediaFiles[type].length - 1;

          // Add preview content based on type
          if (type === "images") {
            const img = document.createElement("img");
            img.src = URL.createObjectURL(file);
            previewContainer.appendChild(img);
          } else if (type === "audios") {
            const audioElement = document.createElement("audio");
            audioElement.src = URL.createObjectURL(file);
            audioElement.controls = true;
            audioElement.style.width = "100%";
            audioElement.style.height = "100%";
            previewContainer.appendChild(audioElement);
          } else if (type === "videos") {
            const videoElement = document.createElement("video");
            videoElement.src = URL.createObjectURL(file);
            videoElement.controls = true;
            videoElement.style.width = "100%";
            videoElement.style.height = "100%";
            previewContainer.appendChild(videoElement);
          }

          // Add remove button
          const removeButton = document.createElement("div");
          removeButton.className = "remove";
          removeButton.textContent = "×";
          removeButton.addEventListener("click", () =>
            removeMedia(previewContainer),
          );
          previewContainer.appendChild(removeButton);

          // Add to preview area
          document
            .getElementById("media-preview")
            .appendChild(previewContainer);
        }
      }

      // Remove media item
      function removeMedia(element) {
        const type = element.dataset.type;
        const index = parseInt(element.dataset.index, 10);

        if (!isNaN(index) && index >= 0 && index < mediaFiles[type].length) {
          // Remove file from array
          mediaFiles[type].splice(index, 1);

          // Remove element from DOM
          element.remove();

          // Update indices of remaining elements
          const remainingElements = document.querySelectorAll(
            `.preview-item[data-type="${type}"]`,
          );
          for (let i = 0; i < remainingElements.length; i++) {
            if (parseInt(remainingElements[i].dataset.index, 10) > index) {
              remainingElements[i].dataset.index =
                parseInt(remainingElements[i].dataset.index, 10) - 1;
            }
          }
        }
      }

      // Send request to server
      document
        .getElementById("send-button")
        .addEventListener("click", async () => {
          const textInput = document.getElementById("text-input").value.trim();
          const returnAudio =
            document.getElementById("audio-output").checked === true;
          const speaker = document.getElementById("speaker-select").value;
          const responseElement = document.getElementById("text-response");
          const audioResponse = document.getElementById("audio-response");
          const audioPlayer = document.getElementById("audio-player");
          const metricsElement = document.getElementById("metrics");

          if (!textInput) {
            alert("Please enter some text before sending.");
            return;
          }

          // Update UI state
          const sendButton = document.getElementById("send-button");
          sendButton.disabled = true;
          sendButton.textContent = "Sending...";
          responseElement.textContent = "Waiting for response...";
          audioResponse.style.display = "none";
          metricsElement.textContent = "";

          // Start timing
          const startTime = Date.now();

          try {
            if (hasMediaFiles()) {
              // Use multimodal endpoint if we have media files
              await sendMultimodalRequest(textInput, returnAudio, speaker);
            } else {
              // Use text-only endpoint if no media files
              await sendTextRequest(textInput, returnAudio, speaker);
            }
          } catch (error) {
            responseElement.textContent = `Error: ${error.message}`;
            console.error("Request error:", error);
          } finally {
            // Update UI state
            sendButton.disabled = false;
            sendButton.textContent = "Send";

            // Calculate and display metrics
            const endTime = Date.now();
            const duration = (endTime - startTime) / 1000;
            metricsElement.textContent = `Response time: ${duration.toFixed(2)} seconds`;
          }
        });

      // Check if we have any media files
      function hasMediaFiles() {
        return (
          mediaFiles.images.length > 0 ||
          mediaFiles.audios.length > 0 ||
          mediaFiles.videos.length > 0
        );
      }

      // Send text-only request
      async function sendTextRequest(text, returnAudio, speaker) {
        const responseElement = document.getElementById("text-response");
        const audioResponse = document.getElementById("audio-response");
        const audioPlayer = document.getElementById("audio-player");
        const systemPrompt = document
          .getElementById("system-prompt")
          .value.trim();

        const requestData = {
          input_text: text, // Changed from "text" to "input_text"
          speaker: speaker,
          return_audio: returnAudio === true,
          max_new_tokens: 1024,
          system_prompt: systemPrompt || null,
        };

        const response = await fetch(API_ENDPOINTS.inference, {
          method: "POST",
          headers: {
            "Content-Type": "application/json",
          },
          body: JSON.stringify(requestData),
        });

        if (!response.ok) {
          throw new Error(
            `Server error: ${response.status} ${response.statusText}`,
          );
        }

        const data = await response.json();
        displayResponse(data, responseElement, audioResponse, audioPlayer);
      }

      // Send multimodal request
      async function sendMultimodalRequest(text, returnAudio, speaker) {
        const responseElement = document.getElementById("text-response");
        const audioResponse = document.getElementById("audio-response");
        const audioPlayer = document.getElementById("audio-player");

        const systemPrompt = document
          .getElementById("system-prompt")
          .value.trim();

        // Create form data
        const formData = new FormData();
        formData.append("input_text", text); // Changed from "text" to "input_text"
        formData.append("speaker", speaker);
        formData.append(
          "return_audio",
          returnAudio === true ? "true" : "false",
        );
        formData.append("max_new_tokens", 1024);
        if (systemPrompt) formData.append("system_prompt", systemPrompt);

        // Add media files
        mediaFiles.images.forEach((file) => formData.append("images", file));
        mediaFiles.audios.forEach((file) => formData.append("audios", file));
        mediaFiles.videos.forEach((file) => formData.append("videos", file));

        // Send request
        const response = await fetch(API_ENDPOINTS.multimodal, {
          method: "POST",
          body: formData,
        });

        if (!response.ok) {
          throw new Error(
            `Server error: ${response.status} ${response.statusText}`,
          );
        }

        const data = await response.json();
        displayResponse(data, responseElement, audioResponse, audioPlayer);
      }

      // Display response data
      function displayResponse(data, textElement, audioElement, audioPlayer) {
        // Your API returns a different format than the client expects
        // Display text response - text is a string in your API, not an array
        textElement.textContent = data.text || "No text response received.";

        // Handle audio response if available
        if (data.has_audio && data.audio_waveform) {
          // Convert the audio waveform array to an audio buffer and then to a blob
          const audioContext = new (window.AudioContext ||
            window.webkitAudioContext)();
          const audioBuffer = audioContext.createBuffer(
            1,
            data.audio_waveform.length,
            data.audio_sample_rate || 24000,
          );
          const channelData = audioBuffer.getChannelData(0);

          // Copy the waveform data to the audio buffer
          for (let i = 0; i < data.audio_waveform.length; i++) {
            channelData[i] = data.audio_waveform[i];
          }

          // Convert to WAV and create an object URL
          audioBufferToWav(audioBuffer)
            .then((wavBlob) => {
              const audioUrl = URL.createObjectURL(wavBlob);
              audioPlayer.src = audioUrl;
              audioElement.style.display = "block";

              // Automatically play audio
              audioPlayer.play().catch((error) => {
                console.warn(
                  "Auto-play failed. This may be due to browser policies:",
                  error,
                );
              });
            })
            .catch((error) => {
              console.error("Error converting audio:", error);
              audioElement.style.display = "none";
            });
        } else {
          audioElement.style.display = "none";
          audioPlayer.src = "";
        }
      }
      async function audioBufferToWav(audioBuffer) {
        const numberOfChannels = audioBuffer.numberOfChannels;
        const length = audioBuffer.length * numberOfChannels * 2;
        const buffer = new ArrayBuffer(44 + length);
        const view = new DataView(buffer);
        const sampleRate = audioBuffer.sampleRate;

        // Write WAV header
        writeString(view, 0, "RIFF");
        view.setUint32(4, 36 + length, true);
        writeString(view, 8, "WAVE");
        writeString(view, 12, "fmt ");
        view.setUint32(16, 16, true);
        view.setUint16(20, 1, true);
        view.setUint16(22, numberOfChannels, true);
        view.setUint32(24, sampleRate, true);
        view.setUint32(28, sampleRate * numberOfChannels * 2, true);
        view.setUint16(32, numberOfChannels * 2, true);
        view.setUint16(34, 16, true);
        writeString(view, 36, "data");
        view.setUint32(40, length, true);

        // Write audio data
        const channelData = [];
        for (let i = 0; i < numberOfChannels; i++) {
          channelData.push(audioBuffer.getChannelData(i));
        }

        let offset = 44;
        for (let i = 0; i < audioBuffer.length; i++) {
          for (let channel = 0; channel < numberOfChannels; channel++) {
            const sample = Math.max(-1, Math.min(1, channelData[channel][i]));
            view.setInt16(
              offset,
              sample < 0 ? sample * 0x8000 : sample * 0x7fff,
              true,
            );
            offset += 2;
          }
        }

        return new Blob([buffer], { type: "audio/wav" });
      }

      function writeString(view, offset, string) {
        for (let i = 0; i < string.length; i++) {
          view.setUint8(offset + i, string.charCodeAt(i));
        }
      }

      // Initialize on page load
      window.addEventListener("load", () => {
        checkConnection();

        document.getElementById("audio-output").checked = true;
      });

      let mediaRecorder;
      let audioChunks = [];
      let isRecording = false;

      document
        .getElementById("record-button")
        .addEventListener("click", toggleRecording);
      async function toggleRecording() {
        const recordButton = document.getElementById("record-button");
        const recordingStatus = document.getElementById("recording-status");

        if (!isRecording) {
          // Start recording
          try {
            const stream = await navigator.mediaDevices.getUserMedia({
              audio: true,
            });
            mediaRecorder = new MediaRecorder(stream);
            audioChunks = [];

            mediaRecorder.ondataavailable = (event) => {
              audioChunks.push(event.data);
            };

            mediaRecorder.onstop = () => {
              // Create audio file from recorded chunks
              const audioBlob = new Blob(audioChunks, { type: "audio/wav" });
              const audioFile = new File([audioBlob], "recorded_audio.wav", {
                type: "audio/wav",
              });

              // Add to media files
              mediaFiles.audios.push(audioFile);

              // Create preview element
              const previewContainer = document.createElement("div");
              previewContainer.className = "preview-item";
              previewContainer.dataset.type = "audios";
              previewContainer.dataset.index = mediaFiles.audios.length - 1;

              const audioElement = document.createElement("audio");
              audioElement.src = URL.createObjectURL(audioFile);
              audioElement.controls = true;
              audioElement.style.width = "100%";
              audioElement.style.height = "100%";
              previewContainer.appendChild(audioElement);

              // Add remove button
              const removeButton = document.createElement("div");
              removeButton.className = "remove";
              removeButton.textContent = "×";
              removeButton.addEventListener("click", () =>
                removeMedia(previewContainer),
              );
              previewContainer.appendChild(removeButton);

              // Add to preview area
              document
                .getElementById("media-preview")
                .appendChild(previewContainer);

              recordingStatus.textContent = "Recording saved";
              setTimeout(() => {
                recordingStatus.textContent = "";
              }, 3000);
            };

            // Start recording
            mediaRecorder.start();
            isRecording = true;
            recordButton.textContent = "⏹️ Stop Recording";
            recordButton.classList.add("recording");
            recordingStatus.textContent = "Recording... (click again to stop)";
          } catch (error) {
            console.error("Error accessing microphone:", error);
            recordingStatus.textContent = `Error: ${error.message}`;
          }
        } else {
          // Stop recording
          if (mediaRecorder && mediaRecorder.state !== "inactive") {
            mediaRecorder.stop();

            // Stop all tracks in the stream
            const tracks = mediaRecorder.stream.getTracks();
            tracks.forEach((track) => track.stop());
          }

          isRecording = false;
          recordButton.textContent = "Record";
          recordButton.classList.remove("recording");
        }
      }

      // WebRTC Variables
      let webrtcPeerConnection;
      let webrtcLocalStream;
      let webrtcWebsocket;
      let webrtcClientId;
      let webrtcAudioContext;
      let webrtcAnalyser;
      let webrtcVisualizerAnimationId;
      let webrtcIsListening = false;

      // DOM Elements
      const webrtcStartButton = document.getElementById("webrtc-start-button");
      const webrtcStopButton = document.getElementById("webrtc-stop-button");
      const webrtcStatusElement = document.getElementById("webrtc-status");
      const webrtcResponsesElement =
        document.getElementById("webrtc-responses");
      const visualizerCanvas = document.getElementById("audio-visualizer");
      const canvasCtx = visualizerCanvas.getContext("2d");

      // Update WebRTC connection status
      function updateWebRTCStatus(message, type) {
        webrtcStatusElement.textContent = message;
        webrtcStatusElement.className = `status ${type}`;
      }

      // Add a response to the WebRTC responses container
      function addWebRTCResponse(text) {
        const responseItem = document.createElement("div");
        responseItem.style.padding = "10px";
        responseItem.style.marginBottom = "10px";
        responseItem.style.backgroundColor = "#f9f9f9";
        responseItem.style.borderRadius = "5px";
        responseItem.style.boxShadow = "0 1px 3px rgba(0,0,0,0.1)";
        responseItem.textContent = text;
        webrtcResponsesElement.appendChild(responseItem);
        webrtcResponsesElement.scrollTop = webrtcResponsesElement.scrollHeight;
      }

      // Draw audio visualizer
      function drawWebRTCVisualizer() {
        const WIDTH = visualizerCanvas.width;
        const HEIGHT = visualizerCanvas.height;

        webrtcVisualizerAnimationId =
          requestAnimationFrame(drawWebRTCVisualizer);

        if (!webrtcAnalyser) return;

        const bufferLength = webrtcAnalyser.frequencyBinCount;
        const dataArray = new Uint8Array(bufferLength);
        webrtcAnalyser.getByteTimeDomainData(dataArray);

        canvasCtx.fillStyle = "#f0f0f0";
        canvasCtx.fillRect(0, 0, WIDTH, HEIGHT);
        canvasCtx.lineWidth = 2;
        canvasCtx.strokeStyle = "#007bff";
        canvasCtx.beginPath();

        const sliceWidth = WIDTH / bufferLength;
        let x = 0;

        for (let i = 0; i < bufferLength; i++) {
          const v = dataArray[i] / 128.0;
          const y = (v * HEIGHT) / 2;

          if (i === 0) {
            canvasCtx.moveTo(x, y);
          } else {
            canvasCtx.lineTo(x, y);
          }

          x += sliceWidth;
        }

        canvasCtx.lineTo(WIDTH, HEIGHT / 2);
        canvasCtx.stroke();
      }

      // Start WebRTC streaming
      async function startWebRTCStreaming() {
        try {
          // Reset the interface
          updateWebRTCStatus("Requesting microphone access...", "processing");
          webrtcStartButton.disabled = true;
          webrtcResponsesElement.innerHTML =
            '<div style="color: #666; font-style: italic;">Real-time inference results will appear here...</div>';

          // Get audio stream
          webrtcLocalStream = await navigator.mediaDevices.getUserMedia({
            audio: {
              echoCancellation: true,
              noiseSuppression: true,
              autoGainControl: true,
            },
            video: false,
          });

          console.log("Microphone access granted");

          // Set up audio context and analyser for visualization
          webrtcAudioContext = new (window.AudioContext ||
            window.webkitAudioContext)();
          const source =
            webrtcAudioContext.createMediaStreamSource(webrtcLocalStream);
          webrtcAnalyser = webrtcAudioContext.createAnalyser();
          webrtcAnalyser.fftSize = 2048;
          source.connect(webrtcAnalyser);

          // Start visualizer
          requestAnimationFrame(drawWebRTCVisualizer);

          // Create peer connection
          webrtcPeerConnection = new RTCPeerConnection({
            iceServers: [{ urls: "stun:stun.l.google.com:19302" }],
          });

          // Add stream to peer connection
          webrtcLocalStream.getTracks().forEach((track) => {
            webrtcPeerConnection.addTrack(track, webrtcLocalStream);
          });

          // Handle ICE connection state changes
          webrtcPeerConnection.oniceconnectionstatechange = () => {
            console.log(
              `ICE connection state: ${webrtcPeerConnection.iceConnectionState}`,
            );

            if (
              webrtcPeerConnection.iceConnectionState === "disconnected" ||
              webrtcPeerConnection.iceConnectionState === "failed" ||
              webrtcPeerConnection.iceConnectionState === "closed"
            ) {
              stopWebRTCStreaming();
            }
          };

          // Create and set local description (offer)
          const offer = await webrtcPeerConnection.createOffer();
          await webrtcPeerConnection.setLocalDescription(offer);
          console.log("Created offer");

          // Send offer to server
          updateWebRTCStatus("Connecting to server...", "processing");
          const response = await fetch(`${API_BASE_URL}/api/webrtc/offer`, {
            method: "POST",
            headers: {
              "Content-Type": "application/json",
            },
            body: JSON.stringify({
              sdp: {
                sdp: webrtcPeerConnection.localDescription.sdp,
                type: webrtcPeerConnection.localDescription.type,
              },
            }),
          });

          if (!response.ok) {
            throw new Error(
              `Server error: ${response.status} ${response.statusText}`,
            );
          }

          const data = await response.json();
          webrtcClientId = data.client_id;

          console.log(`Received answer and client ID: ${webrtcClientId}`);

          // Set remote description (answer)
          const remoteDesc = new RTCSessionDescription({
            sdp: data.sdp.sdp,
            type: data.sdp.type,
          });
          await webrtcPeerConnection.setRemoteDescription(remoteDesc);
          console.log("Set remote description");

          // Connect WebSocket for receiving results
          connectWebRTCWebSocket(webrtcClientId);

          // Update UI
          updateWebRTCStatus("Connected. Waiting for speech...", "connected");
          webrtcStopButton.disabled = false;
        } catch (error) {
          console.error(`Error starting stream: ${error.message}`);
          updateWebRTCStatus(`Error: ${error.message}`, "disconnected");
          stopWebRTCStreaming();
          webrtcStartButton.disabled = false;
        }
      }

      // Connect WebSocket for WebRTC
      function connectWebRTCWebSocket(clientId) {
        const protocol = window.location.protocol;
        const wsProtocol = protocol === "https:" ? "wss:" : "ws:";
        const wsUrl = `${wsProtocol}//${window.location.host}/api/webrtc/ws/${clientId}`;

        webrtcWebsocket = new WebSocket(wsUrl);

        webrtcWebsocket.onopen = () => {
          console.log("WebRTC WebSocket connected");
        };

        webrtcWebsocket.onmessage = (event) => {
          const message = JSON.parse(event.data);
          console.log("WebRTC WebSocket message:", message);

          switch (message.type) {
            case "status":
              if (message.status === "listening") {
                updateWebRTCStatus("Listening...", "listening");
                webrtcIsListening = true;
              } else if (message.status === "processing") {
                updateWebRTCStatus("Processing speech...", "processing");
              }
              break;

            case "result":
              updateWebRTCStatus(
                "Connected. Waiting for speech...",
                "connected",
              );
              if (message.result && message.result.text) {
                addWebRTCResponse(message.result.text);
              }
              break;

            case "error":
              console.error(`WebRTC error: ${message.error}`);
              updateWebRTCStatus(`Error: ${message.error}`, "disconnected");
              break;
          }
        };

        webrtcWebsocket.onclose = () => {
          console.log("WebRTC WebSocket disconnected");
          webrtcWebsocket = null;
        };

        webrtcWebsocket.onerror = (error) => {
          console.error("WebRTC WebSocket error:", error);
        };
      }

      // Stop WebRTC streaming
      function stopWebRTCStreaming() {
        // Close peer connection
        if (webrtcPeerConnection) {
          webrtcPeerConnection.close();
          webrtcPeerConnection = null;
          console.log("WebRTC peer connection closed");
        }

        // Stop all tracks in the stream
        if (webrtcLocalStream) {
          webrtcLocalStream.getTracks().forEach((track) => track.stop());
          webrtcLocalStream = null;
          console.log("WebRTC local stream stopped");
        }

        // Close WebSocket
        if (webrtcWebsocket) {
          webrtcWebsocket.close();
          webrtcWebsocket = null;
          console.log("WebRTC WebSocket closed");
        }

        // Stop audio context
        if (webrtcAudioContext) {
          webrtcAudioContext.close();
          webrtcAudioContext = null;
          webrtcAnalyser = null;
          console.log("WebRTC audio context closed");
        }

        // Cancel visualizer animation
        if (webrtcVisualizerAnimationId) {
          cancelAnimationFrame(webrtcVisualizerAnimationId);
          webrtcVisualizerAnimationId = null;
        }

        // Reset canvas
        if (canvasCtx) {
          canvasCtx.fillStyle = "#f0f0f0";
          canvasCtx.fillRect(
            0,
            0,
            visualizerCanvas.width,
            visualizerCanvas.height,
          );
        }

        // Update UI
        updateWebRTCStatus("Disconnected", "disconnected");
        webrtcStartButton.disabled = false;
        webrtcStopButton.disabled = true;
        webrtcIsListening = false;
      }

      // Event listeners for WebRTC buttons
      webrtcStartButton.addEventListener("click", startWebRTCStreaming);
      webrtcStopButton.addEventListener("click", stopWebRTCStreaming);

      // Clean up WebRTC resources when page unloads
      window.addEventListener("beforeunload", () => {
        stopWebRTCStreaming();
      });
    </script>
  </body>
</html>
